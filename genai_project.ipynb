{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "kaSnYWRfUvcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WKNLkdcUulc",
        "outputId": "358f418a-3a7f-430e-8b3b-eacab955d03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_groq langchain_community langgraph rizaio streamlit faiss-cpu sentence-transformers langsmith pyngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Secure API Key Setup"
      ],
      "metadata": {
        "id": "HrE8MySmU4py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "\n",
        "# Ensure the keys are already set; if not, raise an error.\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    raise ValueError(\"GROQ_API_KEY is not set!\")\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    raise ValueError(\"TAVILY_API_KEY is not set!\")\n",
        "if \"RIZA_API_KEY\" not in os.environ:\n",
        "    raise ValueError(\"RIZA_API_KEY is not set!\")\n",
        "\n",
        "print(\"API keys loaded from environment.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjNLE0_U1A-",
        "outputId": "4dbffa69-f555-4683-e141-d8ff71179122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "pdSMyb1GVBQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile imports.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from typing import Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "from pprint import pprint\n",
        "\n",
        "# LangChain modules and document loaders\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Community tools\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.riza.command import ExecPython\n",
        "\n",
        "# LangGraph modules\n",
        "from langgraph.types import Command\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# For displaying images in notebooks\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# LangSmith for logging/monitoring (if available)\n",
        "try:\n",
        "    import langsmith\n",
        "except ImportError:\n",
        "    print(\"LangSmith not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hcyti3qVBre",
        "outputId": "f2ab61fc-0597-4cfc-e0bc-b70e6a6df72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing imports.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM and Tools Setup"
      ],
      "metadata": {
        "id": "wbplUr8AVHJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm_setup.py\n",
        "from config import *\n",
        "from imports import *\n",
        "from langchain_groq import ChatGroq  # Import the ChatGroq class\n",
        "\n",
        "# Initialize ChatGroq LLM using the API key and the chosen model.\n",
        "llm = ChatGroq(groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "# Initialize community tools:\n",
        "tool_tavily = TavilySearchResults(max_results=2)\n",
        "tool_code_interpreter = ExecPython()\n",
        "\n",
        "# Combine tools into a list (if needed later).\n",
        "tools = [tool_tavily, tool_code_interpreter]\n",
        "\n",
        "print(\"LLM and tools initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVKY_sfVHlG",
        "outputId": "9abaf490-3961-4d19-d92e-f603c99ee5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Shot Learning Functions"
      ],
      "metadata": {
        "id": "07joRvMQVLQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multi_shot_learning.py\n",
        "from imports import *\n",
        "from llm_setup import llm  # Ensure llm is imported\n",
        "\n",
        "def zero_shot_learning(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot: Provide only the query to the LLM.\n",
        "    \"\"\"\n",
        "    prompt = f\"Answer the following question as accurately as possible:\\n\\n{query}\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def one_shot_learning(query: str) -> str:\n",
        "    \"\"\"\n",
        "    One-shot: Provide one example along with the query.\n",
        "    \"\"\"\n",
        "    example_query = \"What is the capital of France?\"\n",
        "    example_answer = \"The capital of France is Paris.\"\n",
        "    prompt = (\n",
        "        f\"Example:\\nQuestion: {example_query}\\nAnswer: {example_answer}\\n\\n\"\n",
        "        f\"Now answer the following question:\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def few_shot_learning(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Few-shot: Provide multiple examples along with the query.\n",
        "    \"\"\"\n",
        "    examples = (\n",
        "        \"Example 1:\\nQuestion: Who wrote '1984'?\\nAnswer: George Orwell wrote '1984'.\\n\\n\"\n",
        "        \"Example 2:\\nQuestion: What is the boiling point of water?\\nAnswer: The boiling point of water is 100Â°C at sea level.\\n\\n\"\n",
        "    )\n",
        "    prompt = (\n",
        "        f\"{examples}\"\n",
        "        f\"Now answer the following question in a similar style:\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "print(\"Multi-shot learning functions defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx-UooAbVLry",
        "outputId": "d4c988b6-2f2c-4332-aa97-07603efaf19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multi_shot_learning.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Integration"
      ],
      "metadata": {
        "id": "wMj-IBoeVf3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_integration.py\n",
        "from imports import *\n",
        "\n",
        "# Use the new (or fallback) document loaders:\n",
        "try:\n",
        "    from langchain_community.document_loaders import WikipediaLoader, ArxivLoader\n",
        "except ImportError:\n",
        "    from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
        "\n",
        "def load_wikipedia_documents(query: str):\n",
        "    \"\"\"\n",
        "    Load documents from Wikipedia based on the given query.\n",
        "    \"\"\"\n",
        "    loader = WikipediaLoader(query)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "def load_arxiv_documents(query: str):\n",
        "    \"\"\"\n",
        "    Load documents from arXiv based on the given query.\n",
        "    \"\"\"\n",
        "    loader = ArxivLoader(query=query, max_results=3)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "def trim_document(doc, max_chars: int = 1000):\n",
        "    \"\"\"\n",
        "    Trim the document's content to a maximum number of characters.\n",
        "    \"\"\"\n",
        "    # Import Document from langchain.schema to create a new document.\n",
        "    try:\n",
        "        from langchain_community.schema import Document\n",
        "    except ImportError:\n",
        "        from langchain.schema import Document\n",
        "    trimmed_content = doc.page_content[:max_chars]\n",
        "    return Document(page_content=trimmed_content, metadata=doc.metadata)\n",
        "\n",
        "def setup_vector_store_from_web(query: str, max_chars_per_doc: int = 1000):\n",
        "    \"\"\"\n",
        "    Create a FAISS vector store by retrieving documents from Wikipedia and arXiv,\n",
        "    and trimming each document's content to a maximum number of characters.\n",
        "    \"\"\"\n",
        "    docs_wiki = load_wikipedia_documents(query)\n",
        "    docs_arxiv = load_arxiv_documents(query)\n",
        "    docs = docs_wiki + docs_arxiv\n",
        "\n",
        "    # Trim each document to the specified maximum number of characters.\n",
        "    trimmed_docs = [trim_document(doc, max_chars=max_chars_per_doc) for doc in docs]\n",
        "\n",
        "    # Use updated embeddings and vectorstore if available:\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "    except ImportError:\n",
        "        from langchain.embeddings import HuggingFaceEmbeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    try:\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "    except ImportError:\n",
        "        from langchain.vectorstores import FAISS\n",
        "    vector_store = FAISS.from_documents(trimmed_docs, embeddings)\n",
        "    return vector_store\n",
        "\n",
        "def create_rag_chain_from_web(query: str, max_chars_per_doc: int = 1000, k: int = 2):\n",
        "    \"\"\"\n",
        "    Create a RetrievalQA chain using a FAISS vector store built from web sources.\n",
        "    \"\"\"\n",
        "    vector_store = setup_vector_store_from_web(query, max_chars_per_doc=max_chars_per_doc)\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,  # Make sure llm is imported from llm_setup.py below.\n",
        "        chain_type=\"stuff\",  # This is a simple chain type for demonstration.\n",
        "        retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def answer_with_rag_from_web(query: str, max_chars_per_doc: int = 1000, k: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Run the RAG chain with the provided query and return the generated answer.\n",
        "    \"\"\"\n",
        "    rag_chain = create_rag_chain_from_web(query, max_chars_per_doc=max_chars_per_doc, k=k)\n",
        "    # Use invoke (the updated method) instead of run.\n",
        "    result = rag_chain.invoke(query)\n",
        "    return result\n",
        "\n",
        "# Ensure that llm is imported from our LLM setup module.\n",
        "from llm_setup import llm\n",
        "\n",
        "print(\"RAG pipeline with Wikipedia and arXiv integration (with document trimming) set up.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTcuqoAdVc2D",
        "outputId": "f0192859-c179-43ad-c615-8a4ba4c5fb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_integration.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervisor Node"
      ],
      "metadata": {
        "id": "L_shGt8oVqho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nodes_supervisor.py\n",
        "from imports import *\n",
        "from llm_setup import llm  # Import llm from llm_setup.py\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "\n",
        "class Supervisor(BaseModel):\n",
        "    next: str  # Expected values: \"enhancer\", \"researcher\", or \"coder\"\n",
        "    reason: str\n",
        "\n",
        "system_prompt_supervisor = (\n",
        "    \"You are a workflow supervisor managing a team of agents: Prompt Enhancer, Researcher, and Coder. \"\n",
        "    \"Analyze the conversation history and decide which agent should handle the next step. \"\n",
        "    \"Choose 'enhancer' for ambiguous queries, 'researcher' for gathering information, or 'coder' for technical tasks.\"\n",
        ")\n",
        "\n",
        "def supervisor_node(state):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt_supervisor}] + state[\"messages\"]\n",
        "    response = llm.with_structured_output(Supervisor).invoke(messages)\n",
        "    goto = response.next\n",
        "    reason = response.reason\n",
        "    print(f\"Supervisor routing to: {goto} because: {reason}\")\n",
        "    return Command(\n",
        "        update={\"messages\": [HumanMessage(content=reason, name=\"supervisor\")]},\n",
        "        goto=goto\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JecnYwJdVnly",
        "outputId": "f104e200-428d-4eba-e8e7-1f30803e8e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nodes_supervisor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhancer Node"
      ],
      "metadata": {
        "id": "66hr_gbAVvo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nodes_enhancer.py\n",
        "from imports import *\n",
        "from llm_setup import llm  # Import llm\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "\n",
        "def enhancer_node(state):\n",
        "    system_prompt_enhancer = (\n",
        "        \"You are an advanced query enhancer. Your task is to refine and clarify the user's input by removing ambiguities. \"\n",
        "        \"Generate a more precise version of the query without asking further questions.\"\n",
        "    )\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt_enhancer}] + state[\"messages\"]\n",
        "    enhanced_query = llm.invoke(messages)\n",
        "    print(\"Enhancer: Query refined.\")\n",
        "    return Command(\n",
        "        update={\"messages\": [HumanMessage(content=enhanced_query.content, name=\"enhancer\")]},\n",
        "        goto=\"supervisor\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PQuBTGaVsv6",
        "outputId": "a4149f32-2ea0-4060-9431-0ec333704b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nodes_enhancer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Researcher Node"
      ],
      "metadata": {
        "id": "gsRrUQWjV0gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nodes_researcher.py\n",
        "from imports import *\n",
        "from llm_setup import llm, tool_tavily  # Import both llm and tool_tavily\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def research_node(state):\n",
        "    research_agent = create_react_agent(\n",
        "        llm,\n",
        "        tools=[tool_tavily],\n",
        "        state_modifier=\"You are a researcher. Focus solely on gathering and summarizing relevant information.\"\n",
        "    )\n",
        "    result = research_agent.invoke(state)\n",
        "    print(\"Researcher: Research completed and summarized.\")\n",
        "    return Command(\n",
        "        update={\"messages\": [HumanMessage(content=result['messages'][-1].content, name=\"researcher\")]},\n",
        "        goto=\"validator\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ8FF1xJVx13",
        "outputId": "20195476-e86b-4279-ed4c-d5656f6f0bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nodes_researcher.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coder Node"
      ],
      "metadata": {
        "id": "pgVluKEgV64C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nodes_coder.py\n",
        "from imports import *\n",
        "from llm_setup import llm, tool_code_interpreter  # Import llm and tool_code_interpreter\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def code_node(state):\n",
        "    code_agent = create_react_agent(\n",
        "        llm,\n",
        "        tools=[tool_code_interpreter],\n",
        "        state_modifier=\"You are a coder and analyst. Focus on technical problem-solving, performing calculations, and executing code as needed.\"\n",
        "    )\n",
        "    result = code_agent.invoke(state)\n",
        "    print(\"Coder: Code execution and technical analysis complete.\")\n",
        "    return Command(\n",
        "        update={\"messages\": [HumanMessage(content=result['messages'][-1].content, name=\"coder\")]},\n",
        "        goto=\"validator\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Km1sGYV3xL",
        "outputId": "4016a73f-4332-4a04-fb8c-90efd28bfd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nodes_coder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validator Node"
      ],
      "metadata": {
        "id": "pjiEqwM9WAGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nodes_validator.py\n",
        "from imports import *\n",
        "from llm_setup import llm  # Import llm\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "\n",
        "class Validator(BaseModel):\n",
        "    next: str  # Expected values: \"supervisor\" or \"FINISH\"\n",
        "    reason: str\n",
        "\n",
        "system_prompt_validator = (\n",
        "    \"You are a workflow validator. Review the initial user question and the final response. \"\n",
        "    \"If the response fully answers the question, respond with 'FINISH'; otherwise, respond with 'supervisor' for further improvement.\"\n",
        ")\n",
        "\n",
        "def validator_node(state):\n",
        "    user_question = state[\"messages\"][0].content\n",
        "    agent_answer = state[\"messages\"][-1].content\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt_validator},\n",
        "        {\"role\": \"user\", \"content\": user_question},\n",
        "        {\"role\": \"assistant\", \"content\": agent_answer},\n",
        "    ]\n",
        "    response = llm.with_structured_output(Validator).invoke(messages)\n",
        "    goto = response.next if response.next != \"FINISH\" else \"__end__\"\n",
        "    decision = \"Finish\" if goto == \"__end__\" else \"Continue via Supervisor\"\n",
        "    print(f\"Validator: Decision: {decision} â€“ Reason: {response.reason}\")\n",
        "    return Command(\n",
        "        update={\"messages\": [HumanMessage(content=response.reason, name=\"validator\")]},\n",
        "        goto=goto\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofdt1kN_V9CP",
        "outputId": "6d3dd191-36e6-46bd-f8e6-37f372c1adcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nodes_validator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "State Graph Setup"
      ],
      "metadata": {
        "id": "CYQWGyq8WFyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile state_graph_setup.py\n",
        "from imports import *\n",
        "from nodes_supervisor import supervisor_node\n",
        "from nodes_enhancer import enhancer_node\n",
        "from nodes_researcher import research_node\n",
        "from nodes_coder import code_node\n",
        "from nodes_validator import validator_node\n",
        "from langgraph.graph import StateGraph, START, MessagesState\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"supervisor\", supervisor_node)\n",
        "builder.add_node(\"enhancer\", enhancer_node)\n",
        "builder.add_node(\"researcher\", research_node)\n",
        "builder.add_node(\"coder\", code_node)\n",
        "builder.add_node(\"validator\", validator_node)\n",
        "\n",
        "builder.add_edge(START, \"supervisor\")\n",
        "graph = builder.compile()\n",
        "\n",
        "print(\"State Graph compiled and ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SEhTtXvWCrA",
        "outputId": "4e616b64-4e77-4347-95f1-69f65390aca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting state_graph_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Visualization"
      ],
      "metadata": {
        "id": "vTRgVHt6WLmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile graph_visualization.py\n",
        "from imports import *\n",
        "from state_graph_setup import graph\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def display_graph_diagram():\n",
        "    \"\"\"\n",
        "    Generate and display the graph diagram as a PNG image.\n",
        "    \"\"\"\n",
        "    graph_diagram = graph.get_graph(xray=True).draw_mermaid_png()\n",
        "    display(Image(graph_diagram))\n",
        "    print(\"Graph diagram displayed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    display_graph_diagram()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I3HyFRfWInK",
        "outputId": "35faa69e-692d-4c8e-b6e9-04b367b2c52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting graph_visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatbot Interface"
      ],
      "metadata": {
        "id": "V6mK11FJWTg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot_app.py\n",
        "from imports import *\n",
        "from state_graph_setup import graph\n",
        "from multi_shot_learning import zero_shot_learning, one_shot_learning, few_shot_learning\n",
        "from rag_integration import answer_with_rag_from_web\n",
        "\n",
        "# Initialize session state for conversation history.\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []  # List of tuples: (role, message)\n",
        "if \"learning_mode\" not in st.session_state:\n",
        "    st.session_state[\"learning_mode\"] = \"multi_agent\"\n",
        "\n",
        "st.title(\"ðŸ¤– Professional Multi-Agent Chatbot\")\n",
        "st.markdown(\"\"\"\n",
        "Welcome to the Professional Multi-Agent Chatbot integrating LangChain, LangGraph, and LangSmith.\n",
        "This chatbot supports:\n",
        "- **Multi-Agent Workflow:** Supervisor, Enhancer, Researcher, Coder, Validator.\n",
        "- **Multi-shot Learning:** Zero-shot, One-shot, Few-shot.\n",
        "- **RAG:** Retrieval Augmented Generation using Wikipedia and arXiv.\n",
        "\"\"\")\n",
        "\n",
        "learning_mode = st.sidebar.radio(\n",
        "    \"Select Learning Mode:\",\n",
        "    (\"multi_agent\", \"zero_shot\", \"one_shot\", \"few_shot\", \"RAG\")\n",
        ")\n",
        "st.session_state[\"learning_mode\"] = learning_mode\n",
        "\n",
        "st.markdown(\"### Conversation\")\n",
        "for role, text in st.session_state[\"messages\"]:\n",
        "    st.markdown(f\"**{role.capitalize()}:** {text}\")\n",
        "\n",
        "user_input = st.text_input(\"Enter your message:\", key=\"user_input\")\n",
        "\n",
        "if st.button(\"Send\") and user_input:\n",
        "    st.session_state[\"messages\"].append((\"user\", user_input))\n",
        "    mode = st.session_state[\"learning_mode\"]\n",
        "    if mode == \"multi_agent\":\n",
        "        inputs = {\"messages\": st.session_state[\"messages\"]}\n",
        "        for output in graph.stream(inputs):\n",
        "            if \"messages\" in output:\n",
        "                for msg in output[\"messages\"]:\n",
        "                    st.session_state[\"messages\"].append((msg.name, msg.content))\n",
        "    elif mode == \"zero_shot\":\n",
        "        answer = zero_shot_learning(user_input)\n",
        "        st.session_state[\"messages\"].append((\"Zero-Shot\", answer))\n",
        "    elif mode == \"one_shot\":\n",
        "        answer = one_shot_learning(user_input)\n",
        "        st.session_state[\"messages\"].append((\"One-Shot\", answer))\n",
        "    elif mode == \"few_shot\":\n",
        "        answer = few_shot_learning(user_input)\n",
        "        st.session_state[\"messages\"].append((\"Few-Shot\", answer))\n",
        "    elif mode == \"RAG\":\n",
        "        answer = answer_with_rag_from_web(user_input)\n",
        "        st.session_state[\"messages\"].append((\"RAG\", answer))\n",
        "    # Try to rerun the app (if supported) to update the UI.\n",
        "    try:\n",
        "        st.experimental_rerun()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "if st.button(\"Clear Conversation\"):\n",
        "    st.session_state[\"messages\"] = []\n",
        "    try:\n",
        "        st.experimental_rerun()\n",
        "    except AttributeError:\n",
        "        pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4YTL8D7WOWz",
        "outputId": "2237ace0-78c5-47f0-bd09-1a3086a08076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatbot_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangSmith Logging Module"
      ],
      "metadata": {
        "id": "q50nzsUpWZ_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile logging_module.py\n",
        "from imports import *\n",
        "try:\n",
        "    from langsmith import Client\n",
        "except ImportError:\n",
        "    Client = None\n",
        "    print(\"LangSmith not available. Skipping logging integration.\")\n",
        "\n",
        "def log_event(event_name: str, details: dict):\n",
        "    \"\"\"\n",
        "    Log an event using LangSmith.\n",
        "    \"\"\"\n",
        "    if Client:\n",
        "        client = Client()\n",
        "        client.log_event(event_name, details)\n",
        "    else:\n",
        "        print(f\"Log: {event_name} - {details}\")\n",
        "\n",
        "print(\"Logging module set up.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXVSaphHWVt_",
        "outputId": "5ce7c96a-adf0-4618-d8ba-e9140b379a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting logging_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility Functions"
      ],
      "metadata": {
        "id": "vpfgp-bnWgOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "def format_message(role: str, message: str) -> str:\n",
        "    \"\"\"\n",
        "    Format a message for display.\n",
        "    \"\"\"\n",
        "    return f\"**{role.capitalize()}**: {message}\"\n",
        "\n",
        "def debug_print(message: str):\n",
        "    \"\"\"\n",
        "    Utility function for debug printing.\n",
        "    \"\"\"\n",
        "    print(f\"[DEBUG] {message}\")\n",
        "\n",
        "print(\"Utility functions loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vJnXyB5Wc5d",
        "outputId": "da179dac-3287-4834-a81a-6cf0d25bdae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Multi-Shot Learning"
      ],
      "metadata": {
        "id": "y5IAwY96WlMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_multi_shot.py\n",
        "from multi_shot_learning import zero_shot_learning, one_shot_learning, few_shot_learning\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"What is the largest planet in our solar system?\"\n",
        "    print(\"Zero-shot:\", zero_shot_learning(query))\n",
        "    print(\"One-shot:\", one_shot_learning(query))\n",
        "    print(\"Few-shot:\", few_shot_learning(query))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV1IF8g7WiKO",
        "outputId": "12375f64-5628-41a3-adca-694d79ac84c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_multi_shot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test RAG Integration"
      ],
      "metadata": {
        "id": "WZopFo9oWsMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia arxiv pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QthgIWCxvFBw",
        "outputId": "79047216-a88e-4e31-9d57-883d5ceb7e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.11/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_rag.py\n",
        "from rag_integration import answer_with_rag_from_web\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"Impact of climate change on agriculture\"\n",
        "    answer = answer_with_rag_from_web(query)\n",
        "    print(\"RAG Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM_PefDqWoLH",
        "outputId": "7918f039-77fe-4f8d-ee0a-bfd29e0a0794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_rag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Documentation"
      ],
      "metadata": {
        "id": "TgpmrpuGWyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# First, kill any existing ngrok tunnels\n",
        "python -c \"from pyngrok import ngrok; ngrok.kill()\"\n",
        "\n",
        "# Then, launch the Streamlit app in the background using nohup.\n",
        "nohup streamlit run chatbot_app.py > streamlit.log 2>&1 &\n",
        "sleep 5\n"
      ],
      "metadata": {
        "id": "zHw9OjN4bYcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok tunnels list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFP1Nj5py0nq",
        "outputId": "aa7006a0-0fad-4702-d82d-72546f8a39b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "USAGE:\n",
            "  ngrok [command] [flags]\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --url baz.ngrok.dev 8080                           # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --url mydomain.com                              # run ngrok with your own custom domain\n",
            "  ngrok http 80 --cidr-allow 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing tunnels before attempting to create a new one.\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your ngrok authtoken (optional if already set via a bash command)\n",
        "ngrok.set_auth_token(\"2spyR1fXNbqSRjhlYoZ8o3ZNViP_5Y8JgUdHRGNRUAJghf1n9\")\n",
        "\n",
        "# Open a new tunnel on port 8501.\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "555XatG7bm-a",
        "outputId": "1dc5491e-2d96-429c-c08b-837c9df29789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://5345-34-91-10-8.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python graph_visualization.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAgdAJ0NltHL",
        "outputId": "b831e7c0-c9ba-4362-9be1-76e41d8b4699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/imports.py:9: LangChainDeprecationWarning: Importing WikipediaLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import WikipediaLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import WikipediaLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:9: LangChainDeprecationWarning: Importing ArxivLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import ArxivLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import ArxivLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:10: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/imports.py:11: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "API keys loaded from environment.\n",
            "LLM and tools initialized.\n",
            "State Graph compiled and ready.\n",
            "<IPython.core.display.Image object>\n",
            "Graph diagram displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_multi_shot.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rpF8NWIeLru",
        "outputId": "d78b9c63-1e9c-493e-95ce-b0604fc244e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/imports.py:9: LangChainDeprecationWarning: Importing WikipediaLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import WikipediaLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import WikipediaLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:9: LangChainDeprecationWarning: Importing ArxivLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import ArxivLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import ArxivLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:10: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/imports.py:11: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "API keys loaded from environment.\n",
            "LLM and tools initialized.\n",
            "Multi-shot learning functions defined.\n",
            "Zero-shot: The largest planet in our solar system is Jupiter. It has a diameter of approximately 142,984 kilometers (88,846 miles), which is more than 11 times the diameter of the Earth. Jupiter is a gas giant planet, primarily composed of hydrogen and helium, and is known for its distinctive banded appearance and massive storms, such as the Great Red Spot.\n",
            "One-shot: The largest planet in our solar system is Jupiter.\n",
            "Few-shot: The largest planet in our solar system is Jupiter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_rag.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mgCgo-9u3dl",
        "outputId": "e22a6095-a789-4c63-9233-69607a2e3e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/imports.py:9: LangChainDeprecationWarning: Importing WikipediaLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import WikipediaLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import WikipediaLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:9: LangChainDeprecationWarning: Importing ArxivLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import ArxivLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import ArxivLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import WikipediaLoader, ArxivLoader\n",
            "/content/imports.py:10: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/imports.py:11: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "API keys loaded from environment.\n",
            "LLM and tools initialized.\n",
            "RAG pipeline with Wikipedia and arXiv integration (with document trimming) set up.\n",
            "/content/rag_integration.py:54: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "2025-02-10 07:35:12.867353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739172912.888402    8024 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739172912.894751    8024 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-10 07:35:12.916458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "RAG Answer: {'query': 'Impact of climate change on agriculture', 'result': 'According to the provided context, climate change has both direct and indirect impacts on agriculture. The direct consequences include:\\n\\n1. Increase in temperature\\n2. Change in rainfall patterns\\n3. Flooding\\n4. Drought\\n\\nThe secondary consequences include:\\n\\n1. Increased weed pressure\\n2. Increased pest pressure\\n3. Increased disease pressure\\n4. Infrastructure damage\\n\\nThese changes are expected to lead to:\\n\\n1. Decline in yield\\n2. Decline in nutrient density in key crops\\n3. Decreased livestock productivity\\n\\nOverall, climate change poses significant challenges to agricultural productivity and costs, particularly in rural communities that are heavily dependent on agriculture.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B02VAG29u8c_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}